{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise with tensorflow and handwritten digits\n",
    "\n",
    "Your task is to extend the above example to work with the 'classic' MNIST dataset, which contains many thousands of handwritten digits. \n",
    "\n",
    "First you should watch the video https://hooktube.com/watch?v=wQ8BIBpya2k, which gives an introduction to Google's Tensorflow - a Python framework helping to build neural networks- and you follow the tutorial on https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist.\n",
    "\n",
    "You have to reproduce their solution and the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Based on above predict method calculate probability of the following 4 shapes:\n",
    "```\n",
    "\"\"\"@@@@@\n",
    "   @...@\n",
    "   @..@@\n",
    "   @...@\n",
    "   @@@@@\"\"\",\n",
    "\"\"\".@@@@\n",
    "   ....@\n",
    "   @@.@@\n",
    "   @....\n",
    "   @@@@@\"\"\",\n",
    "\"\"\"@.@.@\n",
    "   @...@\n",
    "   @@@@@\n",
    "   ....@\n",
    "   ....@\"\"\",\n",
    "\"\"\"@@@@@\n",
    "   @....\n",
    "   @@.@@\n",
    "   @...@\n",
    "   @@@@@\"\"\"\n",
    "```\n",
    "2. Plot out the images above in black and white \"pixels\" like done previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/10000 [00:00<01:46, 93.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:52<00:00, 89.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.   0.   0.   0.   0.   0.01 0.   0.   1.   0.05]\n",
      "2 [0.05 0.   0.98 0.02 0.   0.   0.   0.   0.   0.  ]\n",
      "4 [0.   0.   0.   0.   0.97 0.15 0.   0.   0.   0.01]\n",
      "6 [0.   0.01 0.   0.   0.   0.14 0.96 0.   0.04 0.  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"a step function made continous in order to use calculus (differential)\"\"\"\n",
    "    return 1 / (1 + math.exp(-t)) # t=0 -> 1/2, t=100 -> ~ 1, t=-100 -> ~0\n",
    "\n",
    "def neuron_output(weights, inputs):\n",
    "    \"\"\"reduces all values from input array and weights array to a value between 0 to 1\"\"\"\n",
    "    return sigmoid(dot(weights, inputs)) \n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "# traning data\n",
    "\n",
    "raw_digits = [\n",
    "        \"\"\"@@@@@\n",
    "           @...@\n",
    "           @...@\n",
    "           @...@\n",
    "           @@@@@\"\"\",\n",
    "        \"\"\"..@..\n",
    "           ..@..\n",
    "           ..@..\n",
    "           ..@..\n",
    "           ..@..\"\"\",\n",
    "        \"\"\"@@@@@\n",
    "           ....@\n",
    "           @@@@@\n",
    "           @....\n",
    "           @@@@@\"\"\",\n",
    "        \"\"\"@@@@@\n",
    "           ....@\n",
    "           @@@@@\n",
    "           ....@\n",
    "           @@@@@\"\"\",\n",
    "        \"\"\"@...@\n",
    "           @...@\n",
    "           @@@@@\n",
    "           ....@\n",
    "           ....@\"\"\",\n",
    "        \"\"\"@@@@@\n",
    "           @....\n",
    "           @@@@@\n",
    "           ....@\n",
    "           @@@@@\"\"\",\n",
    "        \"\"\"@@@@@\n",
    "           @....\n",
    "           @@@@@\n",
    "           @...@\n",
    "           @@@@@\"\"\",\n",
    "        \"\"\"@@@@@\n",
    "           ....@\n",
    "           ....@\n",
    "           ....@\n",
    "           ....@\"\"\",\n",
    "        \"\"\"@@@@@\n",
    "           @...@\n",
    "           @@@@@\n",
    "           @...@\n",
    "           @@@@@\"\"\",\n",
    "        \"\"\"@@@@@\n",
    "           @...@\n",
    "           @@@@@\n",
    "           ....@\n",
    "           @@@@@\"\"\",\n",
    "        \"\"\"@.@.@\n",
    "           @...@\n",
    "           @@@@@\n",
    "           ....@\n",
    "           ....@\"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "def make_digit(raw_digit):\n",
    "    \"\"\"transform digit set to using zeros instead of dots\"\"\"\n",
    "    return [1 if c == '@' else 0\n",
    "            for row in raw_digit.split(\"\\n\")\n",
    "            for c in row.strip()]\n",
    "\n",
    "inputs = [make_digit(raw_digit) for raw_digit in raw_digits]\n",
    "\n",
    "targets = [[1 if i == j else 0 for i in range(10)] for j in range(10)]\n",
    "\n",
    "targets.append([0,0,0,0,1,0,0,0,0,0])\n",
    "\n",
    "#train model\n",
    "\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    \"\"\"takes in a neural network\n",
    "    (represented as a list of lists(non-input layers) of lists(neurons) of weights)\n",
    "    and returns the output from forward-propagating the input\"\"\"\n",
    "    outputs = []\n",
    "    # process one layer at a time\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]               # add a bias input\n",
    "        output = [neuron_output(neuron, input_with_bias)   # compute the output\n",
    "            for neuron in layer]                           # for each neuron\n",
    "        outputs.append(output)                             # and remember it\n",
    "        \n",
    "        # then the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "    return outputs\n",
    "\n",
    "def backpropagate(network, input_vector, targets):\n",
    "    \"\"\"\n",
    "    1. Run feed_forward on an input vector to produce the outputs of all the neurons\n",
    "    in the network.\n",
    "    2. This results in an error for each output neuron—the difference between its out‐\n",
    "    put and its target.\n",
    "    3. Compute the gradient of this error as a function of the neuron’s weights, and\n",
    "    adjust its weights in the direction that most decreases the error.\n",
    "    4. “Propagate” these output errors backward to infer errors for the hidden layer.\n",
    "    5. Compute the gradients of these errors and adjust the hidden layer’s weights in the\n",
    "    same manner.\n",
    "    \"\"\"\n",
    "    # We assume a single hidden layer from the network given to feed_forward function\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "    # the output * (1 - output) is from the derivative of sigmoid\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, targets)]\n",
    "    # adjust weights for output layer, one neuron at a time\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "        # focus on the ith output layer neuron\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            # adjust the jth weight based on both\n",
    "            # this neuron's delta and its jth input\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output\n",
    "    # back-propagate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                     dot(output_deltas, [n[i] for n in output_layer])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "    # adjust weights for hidden layer, one neuron at a time\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, input in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input\n",
    "\n",
    "random.seed(0)   # to get repeatable results\n",
    "input_size = 25  # each input is a vector of length 25 (5x5 \"pixels\")\n",
    "\n",
    "num_hidden = 10   # we'll have 5 neurons in the hidden layer\n",
    "output_size = 10 # we need 10 outputs for each input\n",
    "\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for _ in range(input_size + 1)]\n",
    "                 for _ in range(num_hidden)]\n",
    "#print(hidden_layer)\n",
    "\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for _ in range(num_hidden + 1)]\n",
    "                 for _ in range(output_size)]\n",
    "#print(output_layer)\n",
    "\n",
    "# the network starts out with random weights, one hidden layer and one output layer\n",
    "network = [hidden_layer, output_layer]\n",
    "\n",
    "\n",
    "print('Training the network...')\n",
    "\n",
    "# 10,000 iterations seems enough to converge\n",
    "for _ in tqdm(range(10000)):\n",
    "    for input_vector, target_vector in zip(inputs, targets): # inputs is a matrix of 10x25 (ten digits by 25 pixels), target is a one-hot encoded matrix of 10x10 (10 digits by 10 indices where each row has only 1 one and 9 zeroes)\n",
    "        backpropagate(network, input_vector, target_vector)\n",
    "\n",
    "test_set = [\n",
    "    \"\"\"@@@@@\n",
    "       @...@\n",
    "       @..@@\n",
    "       @...@\n",
    "       @@@@@\"\"\",\n",
    "       \n",
    "    \"\"\".@@@@\n",
    "       ....@\n",
    "       @@.@@\n",
    "       @....\n",
    "       @@@@@\"\"\",\n",
    "       \n",
    "    \"\"\"@.@.@\n",
    "       @...@\n",
    "       @@@@@\n",
    "       ....@\n",
    "       ....@\"\"\",\n",
    "       \n",
    "    \"\"\"@@@@@\n",
    "       @....\n",
    "       @@.@@\n",
    "       @...@\n",
    "       @@@@@\"\"\"\n",
    "]\n",
    "\n",
    "test_set = [make_digit(i) for i in test_set]\n",
    "\n",
    "def predict(in_put):\n",
    "    return feed_forward(network, in_put)[-1]\n",
    "\n",
    "for test_data in test_set:\n",
    "    result = predict(test_data)\n",
    "    result = np.array(result)\n",
    "    print(np.argmax(result), np.array_str(result, precision=2, suppress_small=True))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
